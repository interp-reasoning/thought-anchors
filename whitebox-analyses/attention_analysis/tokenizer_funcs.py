from functools import cache
from typing import List, Tuple, Union
from abc import ABC, abstractmethod
from transformers import AutoTokenizer


class BaseTokenizerAdapter(ABC):
    """Abstract base class for tokenizer adapters."""

    def __init__(self, model_path: str):
        self._tokenizer = None
        self._model_path = model_path

    @property
    def tokenizer(self):
        """Lazy load tokenizer."""
        if self._tokenizer is None:
            self._tokenizer = self._load_tokenizer()
        return self._tokenizer

    @abstractmethod
    def _load_tokenizer(self):
        """Load the tokenizer for this model."""
        pass

    def get_raw_tokens(self, text: str) -> List[str]:
        """Convert text to raw tokens."""
        tokens_int = self.tokenizer.encode(text)
        tokens_words = self.tokenizer.convert_ids_to_tokens(tokens_int)
        return tokens_words

    def tokens_to_clean(self, tokens: Union[List[str], List[int]]) -> str:
        """Convert tokens back to clean text."""
        if tokens and isinstance(tokens[0], str):
            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        else:
            token_ids = tokens
        clean_text = self.tokenizer.decode(token_ids)
        return clean_text


class QwenTokenizerAdapter(BaseTokenizerAdapter):
    """Tokenizer adapter for Qwen models."""

    @cache
    def _load_tokenizer(self):
        return AutoTokenizer.from_pretrained(self._model_path)


class LlamaTokenizerAdapter(BaseTokenizerAdapter):
    """Tokenizer adapter for Llama models."""

    @cache
    def _load_tokenizer(self):
        return AutoTokenizer.from_pretrained(self._model_path)


class TokenizerFactory:
    """Factory for creating tokenizer adapters based on model name."""

    # Model name to HuggingFace path mapping
    MODEL_PATHS = {
        "qwen-14b-base": "Qwen/Qwen2.5-14B",
        "qwen-14b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "qwen-15b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "qwen3": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",  # Assuming same as qwen-15b
        "llama": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "llama-8b": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "gpt-oss": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",  # Uses Llama tokenizer
        "deepseek-r1": "/workspace/models/DeepSeek-R1",  # Local path
    }

    _instances = {}  # Cache adapter instances

    @classmethod
    def get_adapter(cls, model_name: str) -> BaseTokenizerAdapter:
        """Get or create a tokenizer adapter for the given model."""
        if model_name in cls._instances:
            return cls._instances[model_name]

        # Determine model type and path
        model_name_lower = model_name.lower()

        if "qwen" in model_name_lower or "qwq" in model_name_lower:
            if model_name == "qwen-14b-base":
                model_path = cls.MODEL_PATHS["qwen-14b-base"]
            elif "qwen3" in model_name_lower:
                model_path = cls.MODEL_PATHS["qwen3"]
            elif model_name == "qwen-14b":
                model_path = cls.MODEL_PATHS["qwen-14b"]
            else:
                # Default to qwen-15b path for other qwen variants
                model_path = cls.MODEL_PATHS["qwen-15b"]
            adapter = QwenTokenizerAdapter(model_path)

        elif "llama" in model_name_lower:
            model_path = cls.MODEL_PATHS["llama"]
            adapter = LlamaTokenizerAdapter(model_path)

        elif "gpt-oss" in model_name_lower:
            # GPT-OSS uses Llama tokenizer
            model_path = cls.MODEL_PATHS["gpt-oss"]
            adapter = LlamaTokenizerAdapter(model_path)

        elif "deepseek-r1" in model_name_lower:
            model_path = cls.MODEL_PATHS["deepseek-r1"]
            adapter = LlamaTokenizerAdapter(model_path)  # Assuming it uses similar tokenizer

        else:
            raise ValueError(f"Unknown model: {model_name}")

        cls._instances[model_name] = adapter
        return adapter


# Public API functions for backward compatibility
def get_tokenizer(model_name: str) -> AutoTokenizer:
    """
    Get tokenizer for a specific model.

    Args:
        model_name: Name of the model

    Returns:
        AutoTokenizer instance for the model
    """
    adapter = TokenizerFactory.get_adapter(model_name)
    return adapter.tokenizer


def get_raw_tokens(text: str, model_name: str) -> List[str]:
    """
    Get raw tokens for text using specified model's tokenizer.

    Args:
        text: Text to tokenize
        model_name: Name of the model

    Returns:
        List of token strings
    """
    adapter = TokenizerFactory.get_adapter(model_name)
    return adapter.get_raw_tokens(text)


def tokens_to_clean(tokens: Union[List[str], List[int]], model_name: str) -> str:
    """
    Convert tokens back to clean text.

    Args:
        tokens: List of token strings or token IDs
        model_name: Name of the model

    Returns:
        Decoded text string
    """
    adapter = TokenizerFactory.get_adapter(model_name)
    return adapter.tokens_to_clean(tokens)
